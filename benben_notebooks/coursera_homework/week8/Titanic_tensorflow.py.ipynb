{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from tf_utils import random_mini_batches, convert_to_one_hot\n",
    "\n",
    "seed = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTreatmentCabin(x):\n",
    "    try:\n",
    "        isinstance(float(x),(float))\n",
    "        return '0'\n",
    "    except:\n",
    "        return x[0]\n",
    "    \n",
    "def preTreatmentData(treatment_data, with_label = True):\n",
    "    # sex\n",
    "    treatment_data['Sex'] = treatment_data['Sex'].replace(['male', 'female'],[0, 1])\n",
    "\n",
    "    # age\n",
    "    average_age = np.average(list(filter(lambda x: not np.isnan(x), treatment_data['Age'])))\n",
    "    treatment_data['Age'] = np.nan_to_num(treatment_data['Age'], average_age)\n",
    "\n",
    "    # Embarked\n",
    "    data_embarked_unique = {}.fromkeys(treatment_data['Embarked']).keys()\n",
    "    treatment_data['Embarked'] = treatment_data['Embarked'].replace(data_embarked_unique, np.arange(len(data_embarked_unique)))\n",
    "\n",
    "    # Cabin\n",
    "    data_cabin_string_list = list(map(preTreatmentCabin, list(treatment_data['Cabin'])))\n",
    "    data_cabin_keys = {}.fromkeys(data_cabin_string_list).keys()\n",
    "    treatment_data['Cabin'] = data_cabin_string_list\n",
    "    treatment_data['Cabin'] = treatment_data['Cabin'].replace(data_cabin_keys, np.arange(len(data_cabin_keys)))\n",
    "\n",
    "    # print(data)\n",
    "    model_data = treatment_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]\n",
    "    pre_treatment_data = model_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "    if with_label:\n",
    "        pre_treatment_data['Survived'] = treatment_data['Survived']\n",
    "    \n",
    "    return pre_treatment_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (891, 9)\n",
      "train_data shape: (712, 9)\n",
      "test_data shape: (179, 9)\n",
      "train_set shape: (8, 712)\n",
      "train_label shape: (1, 712)\n",
      "test_set shape: (8, 179)\n",
      "test_label shape: (1, 179)\n"
     ]
    }
   ],
   "source": [
    "# train_data\n",
    "raw_train_data = pd.read_csv('train.csv')\n",
    "train_data = preTreatmentData(raw_train_data)\n",
    "\n",
    "# test_data\n",
    "# raw_test_data = pd.read_csv('test.csv')\n",
    "\n",
    "data = train_data.values\n",
    "print(\"data shape:\", data.shape)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "\n",
    "train_set = train_data[:, 0:8].T\n",
    "train_label = train_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"train_set shape:\", train_set.shape)\n",
    "print(\"train_label shape:\", train_label.shape)\n",
    "\n",
    "test_set = test_data[:, 0:8].T\n",
    "test_label = test_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"test_set shape:\", test_set.shape)\n",
    "print(\"test_label shape:\", test_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_sigmoid(train_data, train_label, test_data, test_label, \n",
    "         learning_rate = 0.005, batch_size = 32, \n",
    "         n_epochs = 1000, seed = 3,\n",
    "         threshold = 0.9, show_epoch_cost = False,\n",
    "         is_train = True,\n",
    "         layers = [10, 5]\n",
    "        ):\n",
    "    X = tf.placeholder(dtype = np.float32, shape = [None, 8])\n",
    "    Y = tf.placeholder(dtype = np.float32, shape = [None, 1])\n",
    "    \n",
    "    hidden_layer = tf.layers.dense(inputs=X, units=8, activation=tf.nn.relu)\n",
    "    \n",
    "    for layer in layers:\n",
    "        weight_l2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "        hidden_layer = tf.layers.dense(inputs=hidden_layer, \n",
    "                                       units=layer, \n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_regularizer=weight_l2_regularizer,\n",
    "#                                        bias_regularizer=None,\n",
    "#                                        activity_regularizer=None,\n",
    "                                      )\n",
    "\n",
    "    logits= tf.layers.dense(inputs=hidden_layer, units=1, activation=None)\n",
    "    \n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    \n",
    "    cost = tf.reduce_mean(entropy) + 0.01 * sum(reg_losses)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter('../tmp', sess.graph)\n",
    "        start_time = time.time()\n",
    "        sess.run(tf.global_variables_initializer())\t\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            epoch_cost = 0        \n",
    "            n_batches = int(train_set.shape[1]/batch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(train_set, train_label, batch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X.T, Y: minibatch_Y.T})\n",
    "\n",
    "                epoch_cost += minibatch_cost / n_batches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if epoch % 100 == 0 and show_epoch_cost:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if epoch % 5 == 0 and show_epoch_cost:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')\n",
    "\n",
    "        print('test the model:')\n",
    "\n",
    "        result = tf.cast(tf.greater(logits, threshold), \"float\")\n",
    "        correct_prediction = tf.equal(result , Y)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_set.T, Y: train_label.T}))\n",
    "        \n",
    "        if is_train:\n",
    "            print (\"Test Accuracy:\", accuracy.eval({X: test_set.T, Y: test_label.T}))\n",
    "        else: \n",
    "            predict_label = result.eval({X: test_set.T})\n",
    "            print (\"result Y:\", predict_label.T)  \n",
    "            \n",
    "        print (\"-------------Done----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.674377\n",
      "Cost after epoch 100: 0.337653\n",
      "Cost after epoch 200: 0.285627\n",
      "Cost after epoch 300: 0.261568\n",
      "Cost after epoch 400: 0.269086\n",
      "Total time: 12.2080979347229 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.907303\n",
      "Test Accuracy: 0.72067\n",
      "-------------Done----------------------\n"
     ]
    }
   ],
   "source": [
    "mode_sigmoid(train_data, train_label, test_data, test_label, \n",
    "     show_epoch_cost = True,\n",
    "     learning_rate=0.005, \n",
    "     n_epochs=500, \n",
    "     layers=[8, 16, 32, 16, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> model with learning rate:  0.015\n",
      "Total time: 95.87288856506348 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.863764\n",
      "Test Accuracy: 0.72067\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.014\n",
      "Total time: 100.58641791343689 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.876405\n",
      "Test Accuracy: 0.776536\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.013\n",
      "Total time: 107.92197942733765 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.869382\n",
      "Test Accuracy: 0.765363\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.012\n",
      "Total time: 112.46878099441528 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.889045\n",
      "Test Accuracy: 0.782123\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.011\n",
      "Total time: 115.45625114440918 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.856742\n",
      "Test Accuracy: 0.765363\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.01\n",
      "Total time: 122.77327585220337 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.893258\n",
      "Test Accuracy: 0.748603\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.009\n",
      "Total time: 126.30864834785461 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.898876\n",
      "Test Accuracy: 0.782123\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.008\n",
      "Total time: 130.13495802879333 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.905899\n",
      "Test Accuracy: 0.726257\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.007\n"
     ]
    }
   ],
   "source": [
    "learning_rates = np.linspace(0.015, 0.005, 11)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(\">>> model with learning rate: \", lr)\n",
    "    mode_sigmoid(train_data, train_label, test_data, test_label, \n",
    "         learning_rate = lr, \n",
    "         n_epochs=2000,\n",
    "         layers=[8, 16, 32, 64, 32, 16, 8])\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (891, 9)\n",
      "train_data shape: (712, 9)\n",
      "test_data shape: (179, 9)\n",
      "train_set shape: (8, 712)\n",
      "train_label shape: (1, 712)\n",
      "test_set shape: (8, 418)\n"
     ]
    }
   ],
   "source": [
    "# train_data\n",
    "raw_train_data = pd.read_csv('train.csv')\n",
    "train_data = preTreatmentData(raw_train_data)\n",
    "\n",
    "data = train_data.values\n",
    "print(\"data shape:\", data.shape)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "\n",
    "train_set = train_data[:, 0:8].T\n",
    "train_label = train_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"train_set shape:\", train_set.shape)\n",
    "print(\"train_label shape:\", train_label.shape)\n",
    "\n",
    "# test_data\n",
    "raw_test_data = pd.read_csv('test.csv')\n",
    "test_data = preTreatmentData(raw_test_data, with_label = False)\n",
    "\n",
    "test_set = test_data.T\n",
    "print(\"test_set shape:\", test_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 42.88404107093811 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.907303\n",
      "result Y: [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "-------------Done----------------------\n"
     ]
    }
   ],
   "source": [
    "mode_sigmoid(train_data, train_label, test_data, test_label, \n",
    "     learning_rate = 0.009, \n",
    "     n_epochs=2000, \n",
    "     is_train=False,\n",
    "     layers=[8, 16, 32, 64, 32, 16, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_survived = np.array([  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  1,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  1,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  1,  0,  0,  1,  0,  0,  0,  1,  0,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  1,  0,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  1,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,  1,  0,  0,  1,  1,  0,  1,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  0,  0,  0,  1,  0,  0,  1,  1,  0,  1,  1,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0])\n",
    "\n",
    "predict_label = pd.DataFrame()\n",
    "predict_label['Survived'] = is_survived\n",
    "predict_label['PassengerId'] = raw_test_data['PassengerId']\n",
    "predict_label.to_csv('predict_label.csv',index=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_softmax(train_data, train_label, test_data, test_label, \n",
    "         learning_rate = 0.005, batch_size = 32, \n",
    "         n_epochs = 1000, seed = 3,\n",
    "         threshold = 0.9, show_epoch_cost = False,\n",
    "         is_train = True,\n",
    "         layers = [10, 5]\n",
    "        ):\n",
    "    train_onehot_label = convert_to_one_hot(train_label.astype(int), 2)\n",
    "    test_onehot_label = convert_to_one_hot(test_label.astype(int), 2)\n",
    "    \n",
    "    X = tf.placeholder(dtype = np.float32, shape = [None, 8])\n",
    "    Y = tf.placeholder(dtype = np.float32, shape = [None, 2])\n",
    "    \n",
    "    hidden_layer = tf.layers.dense(inputs=X, units=8, activation=tf.nn.relu)\n",
    "    \n",
    "    for layer in layers:\n",
    "        weight_l2_regularizer = tf.contrib.layers.l2_regularizer(0.001)\n",
    "        hidden_layer = tf.layers.dense(inputs=hidden_layer, \n",
    "                                       units=layer, \n",
    "                                       activation=tf.nn.relu,\n",
    "                                       kernel_regularizer=weight_l2_regularizer,\n",
    "                                      )\n",
    "\n",
    "    logits= tf.layers.dense(inputs=hidden_layer, units=2, activation=None)\n",
    "    \n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels = Y, logits = logits)\n",
    "    \n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    \n",
    "    cost = tf.reduce_mean(entropy) + 0.01 * sum(reg_losses)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter('../tmp', sess.graph)\n",
    "        start_time = time.time()\n",
    "        sess.run(tf.global_variables_initializer())\t\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            epoch_cost = 0        \n",
    "            n_batches = int(train_set.shape[1]/batch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(train_set, train_onehot_label, batch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X.T, Y: minibatch_Y.T})\n",
    "\n",
    "                epoch_cost += minibatch_cost / n_batches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if epoch % 100 == 0 and show_epoch_cost:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if epoch % 5 == 0 and show_epoch_cost:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')\n",
    "\n",
    "        print('test the model:')\n",
    "\n",
    "        result = tf.cast(tf.greater(logits, threshold), \"float\")\n",
    "        correct_prediction = tf.equal(result , Y)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_set.T, Y: train_onehot_label.T}))\n",
    "        \n",
    "        if is_train:\n",
    "            print (\"Test Accuracy:\", accuracy.eval({X: test_set.T, Y: test_onehot_label.T}))\n",
    "        else: \n",
    "            predict_label = result.eval({X: test_set.T})\n",
    "            print (\"result Y:\", predict_label.T)  \n",
    "            \n",
    "        print (\"-------------Done----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.691060\n",
      "Cost after epoch 100: 0.369542\n",
      "Cost after epoch 200: 0.329291\n",
      "Cost after epoch 300: 0.319789\n",
      "Cost after epoch 400: 0.334665\n",
      "Cost after epoch 500: 0.302398\n",
      "Cost after epoch 600: 0.291642\n",
      "Cost after epoch 700: 0.300205\n",
      "Cost after epoch 800: 0.288355\n",
      "Cost after epoch 900: 0.278697\n",
      "Cost after epoch 1000: 0.273979\n",
      "Cost after epoch 1100: 0.273351\n",
      "Cost after epoch 1200: 0.279642\n",
      "Cost after epoch 1300: 0.253140\n",
      "Cost after epoch 1400: 0.256041\n",
      "Cost after epoch 1500: 0.263426\n",
      "Cost after epoch 1600: 0.263023\n",
      "Cost after epoch 1700: 0.266802\n",
      "Cost after epoch 1800: 0.240990\n",
      "Cost after epoch 1900: 0.244029\n",
      "Total time: 48.33601117134094 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.883427\n",
      "Test Accuracy: 0.756983\n",
      "-------------Done----------------------\n"
     ]
    }
   ],
   "source": [
    "mode_softmax(train_data, train_label, test_data, test_label, \n",
    "     show_epoch_cost = True,\n",
    "     learning_rate=0.003, \n",
    "     n_epochs=2000, \n",
    "     layers=[8, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train Accuracy: 0.836376\n",
    "Test Accuracy: 0.72067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> model with learning rate:  0.007\n",
      "Total time: 65.11208963394165 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.833567\n",
      "Test Accuracy: 0.662011\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.006\n",
      "Total time: 66.1773328781128 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.661517\n",
      "Test Accuracy: 0.600559\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.005\n",
      "Total time: 71.32303214073181 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.929775\n",
      "Test Accuracy: 0.740223\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.004\n",
      "Total time: 70.33622980117798 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.902388\n",
      "Test Accuracy: 0.75419\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.003\n",
      "Total time: 74.8505916595459 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.804073\n",
      "Test Accuracy: 0.648045\n",
      "-------------Done----------------------\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = np.linspace(0.007, 0.003, 5)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(\">>> model with learning rate: \", lr)\n",
    "    mode_softmax(train_data, train_label, test_data, test_label, \n",
    "         learning_rate = lr, \n",
    "         n_epochs=2000,\n",
    "         layers=[20, 16, 8])\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_softmax(train_data, train_label, test_data, test_label, \n",
    "     learning_rate = 0.009, \n",
    "     n_epochs=2000, \n",
    "     is_train=False,\n",
    "     layers=[8, 16, 32, 64, 32, 16, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_survived = np.array([  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  1,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,  0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  1,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  1,  0,  0,  1,  0,  0,  0,  1,  0,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  1,  0,  0,  1,  1,  1,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  1,  1,  1,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,  1,  0,  0,  1,  1,  0,  1,  1,  0,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  0,  1,  0,  1,  1,  1,  0,  0,  0,  1,  0,  0,  1,  1,  0,  1,  1,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0])\n",
    "\n",
    "predict_label = pd.DataFrame()\n",
    "predict_label['Survived'] = is_survived\n",
    "predict_label['PassengerId'] = raw_test_data['PassengerId']\n",
    "predict_label.to_csv('predict_label.csv',index=None)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
