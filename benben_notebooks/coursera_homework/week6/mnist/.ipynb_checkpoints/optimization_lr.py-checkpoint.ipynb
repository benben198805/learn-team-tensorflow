{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('../data/mnist', one_hot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe2d8954e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt;  \n",
    "# print('data:', mnist.test.images[1])\n",
    "example = mnist.test.images[1]\n",
    "example = example.reshape((28, 28))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(example)  \n",
    "\n",
    "print('label:',mnist.test.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1.7224326615844852\n",
      "Average loss epoch 1: 0.43431359710115375\n",
      "Average loss epoch 2: 0.36113729458172006\n",
      "Average loss epoch 3: 0.3303489679153705\n",
      "Average loss epoch 4: 0.31729314604769776\n",
      "Average loss epoch 5: 0.3045820072987974\n",
      "Average loss epoch 6: 0.29662010603365085\n",
      "Average loss epoch 7: 0.2933259092636042\n",
      "Average loss epoch 8: 0.28949891261773786\n",
      "Average loss epoch 9: 0.2827778782669481\n",
      "[array([[ 0.65119135,  0.57683337,  0.18013728, ...,  0.1803472 ,\n",
      "         0.64853656,  0.26352227],\n",
      "       [ 0.58186078,  0.0686866 ,  0.56054699, ...,  0.03029239,\n",
      "         0.66838348,  0.93156648],\n",
      "       [ 0.98359787,  0.80355775,  0.65189707, ...,  0.98717546,\n",
      "         0.41403079,  0.05962741],\n",
      "       ..., \n",
      "       [ 0.77490592,  0.27797723,  0.48387015, ...,  0.54676986,\n",
      "         0.19933355,  0.35922968],\n",
      "       [ 0.90761292,  0.67390954,  0.17492986, ...,  0.74403679,\n",
      "         0.99096549,  0.35870218],\n",
      "       [ 0.54745293,  0.17515492,  0.86524522, ...,  0.07254374,\n",
      "         0.73648858,  0.2221638 ]], dtype=float32), array([[-0.22954105,  0.15849504, -0.2911993 , ..., -0.04486624,\n",
      "        -0.31179953, -0.20579839],\n",
      "       [-0.15153939,  0.20956869, -0.31832033, ..., -0.11300789,\n",
      "        -0.29847035, -0.14396107],\n",
      "       [-0.20774342,  0.10004273, -0.32168472, ..., -0.12195112,\n",
      "        -0.32556114, -0.13131782],\n",
      "       ..., \n",
      "       [-0.18924138,  0.1361599 , -0.26156783, ..., -0.11970256,\n",
      "        -0.29454562, -0.17075904],\n",
      "       [-0.12648539,  0.15821417, -0.36229709, ..., -0.04653639,\n",
      "        -0.30203098, -0.20108131],\n",
      "       [-0.16360824,  0.1543735 , -0.27010345, ..., -0.11676734,\n",
      "        -0.35447302, -0.13440317]], dtype=float32)]\n",
      "Total time: 6.150548219680786 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name=\"image\")\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name=\"label\")\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# weights and biases are initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = X * w + b\n",
    "# shape of b depends on Y\n",
    "# tf.Variable(initial_value=None, trainable=True, collections=None,\n",
    "# name=None, dtype=None, ...)\n",
    "\n",
    "# X [1,128] -> Y [1,128]\n",
    "\n",
    "#  n * 50  50 x 128 \n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([784, 10]))\n",
    "\n",
    "b = tf.Variable(tf.zeros([batch_size, 10]))\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "# to get the probability distribution of possible label of the image\n",
    "# DO NOT DO SOFTMAX HERE\n",
    "# n * 784 * 784 * 10 = n * 10 \n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy loss of the real labels with the softmax of logits\n",
    "# use the method:\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, Y)\n",
    "# then use tf.reduce_mean to get the mean loss of the batch\n",
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "\n",
    "# computes the mean over all the examples in the batch\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent to minimize loss\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(loss);\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss);\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        total_loss = 0\n",
    "\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # TO-DO: run optimizer + fetch loss_batch\n",
    "            _, loss_batch = sess.run([optimizer, loss], feed_dict={X:X_batch, Y:Y_batch})\n",
    "            # \n",
    "            total_loss += loss_batch\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "    \n",
    "    print(sess.run([W, b]))\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "    # test the model\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同优化方法的准确率\n",
    "* GradientDescentOptimizer:0.8429\n",
    "* AdadeltaOptimizer:0.7062\n",
    "* AdagradOptimizer:0.851\n",
    "* AdamOptimizer:0.924\n",
    "* FtrlOptimizer:0.8507\n",
    "* RMSPropOptimizer:0.9249\n",
    "\n",
    "## RMSPropOptimizer 不同学习率优化结果\n",
    "* 0.001: 0.9241\n",
    "* 0.005: 0.9258\n",
    "* 0.01:  0.9249\n",
    "\n",
    "\n",
    "## RMSPropOptimizer 不同迭代次数的结果\n",
    "learning_rate = 0.01\n",
    "\n",
    "* 10次：0.9241\n",
    "* 30次：0.9255\n",
    "* 100次：0.926"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
