{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "url = 'https://raw.githubusercontent.com/chiphuyen/tf-stanford-tutorials/master/data/'\n",
    "\n",
    "def maybe_download(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sbp', 'tobacco', 'ldl', 'adiposity', 'famhist', 'typea', 'obesity',\n",
      "       'alcohol', 'age'],\n",
      "      dtype='object')\n",
      "     sbp  tobacco    ldl  adiposity  famhist  typea  obesity  alcohol  age\n",
      "0    160    12.00   5.73      23.11  Present     49    25.30    97.20   52\n",
      "1    144     0.01   4.41      28.61   Absent     55    28.87     2.06   63\n",
      "2    118     0.08   3.48      32.28  Present     52    29.14     3.81   46\n",
      "3    170     7.50   6.41      38.03  Present     51    31.99    24.26   58\n",
      "4    134    13.60   3.50      27.78  Present     60    25.99    57.34   49\n",
      "5    132     6.20   6.47      36.21  Present     62    30.77    14.14   45\n",
      "6    142     4.05   3.38      16.20   Absent     59    20.81     2.62   38\n",
      "7    114     4.08   4.59      14.60  Present     62    23.11     6.72   58\n",
      "8    114     0.00   3.83      19.40  Present     49    24.86     2.49   29\n",
      "9    132     0.00   5.80      30.96  Present     69    30.11     0.00   53\n",
      "10   206     6.00   2.95      32.27   Absent     72    26.81    56.06   60\n",
      "11   134    14.10   4.44      22.39  Present     65    23.09     0.00   40\n",
      "12   118     0.00   1.88      10.05   Absent     59    21.57     0.00   17\n",
      "13   132     0.00   1.87      17.21   Absent     49    23.63     0.97   15\n",
      "14   112     9.65   2.29      17.20  Present     54    23.53     0.68   53\n",
      "15   117     1.53   2.44      28.95  Present     35    25.89    30.03   46\n",
      "16   120     7.50  15.33      22.00   Absent     60    25.31    34.49   49\n",
      "17   146    10.50   8.29      35.36  Present     78    32.73    13.89   53\n",
      "18   158     2.60   7.46      34.07  Present     61    29.30    53.28   62\n",
      "19   124    14.00   6.23      35.96  Present     45    30.09     0.00   59\n",
      "20   106     1.61   1.74      12.32   Absent     74    20.92    13.37   20\n",
      "21   132     7.90   2.85      26.50  Present     51    26.16    25.71   44\n",
      "22   150     0.30   6.38      33.99  Present     62    24.64     0.00   50\n",
      "23   138     0.60   3.81      28.66   Absent     54    28.70     1.46   58\n",
      "24   142    18.20   4.34      24.38   Absent     61    26.19     0.00   50\n",
      "25   124     4.00  12.42      31.29  Present     54    23.23     2.06   42\n",
      "26   118     6.00   9.65      33.91   Absent     60    38.80     0.00   48\n",
      "27   145     9.10   5.24      27.55   Absent     59    20.96    21.60   61\n",
      "28   144     4.09   5.55      31.40  Present     60    29.43     5.55   56\n",
      "29   146     0.00   6.62      25.69   Absent     60    28.07     8.23   63\n",
      "..   ...      ...    ...        ...      ...    ...      ...      ...  ...\n",
      "432  136     0.00   4.00      19.06   Absent     40    21.94     2.06   16\n",
      "433  120     0.00   2.46      13.39   Absent     47    22.01     0.51   18\n",
      "434  132     0.00   3.55       8.66  Present     61    18.50     3.87   16\n",
      "435  136     0.00   1.77      20.37   Absent     45    21.51     2.06   16\n",
      "436  138     0.00   1.86      18.35  Present     59    25.38     6.51   17\n",
      "437  138     0.06   4.15      20.66   Absent     49    22.59     2.49   16\n",
      "438  130     1.22   3.30      13.65   Absent     50    21.40     3.81   31\n",
      "439  130     4.00   2.40      17.42   Absent     60    22.05     0.00   40\n",
      "440  110     0.00   7.14      28.28   Absent     57    29.00     0.00   32\n",
      "441  120     0.00   3.98      13.19  Present     47    21.89     0.00   16\n",
      "442  166     6.00   8.80      37.89   Absent     39    28.70    43.20   52\n",
      "443  134     0.57   4.75      23.07   Absent     67    26.33     0.00   37\n",
      "444  142     3.00   3.69      25.10   Absent     60    30.08    38.88   27\n",
      "445  136     2.80   2.53       9.28  Present     61    20.70     4.55   25\n",
      "446  142     0.00   4.32      25.22   Absent     47    28.92     6.53   34\n",
      "447  130     0.00   1.88      12.51  Present     52    20.28     0.00   17\n",
      "448  124     1.80   3.74      16.64  Present     42    22.26    10.49   20\n",
      "449  144     4.00   5.03      25.78  Present     57    27.55    90.00   48\n",
      "450  136     1.81   3.31       6.74   Absent     63    19.57    24.94   24\n",
      "451  120     0.00   2.77      13.35   Absent     67    23.37     1.03   18\n",
      "452  154     5.53   3.20      28.81  Present     61    26.15    42.79   42\n",
      "453  124     1.60   7.22      39.68  Present     36    31.50     0.00   51\n",
      "454  146     0.64   4.82      28.02   Absent     60    28.11     8.23   39\n",
      "455  128     2.24   2.83      26.48   Absent     48    23.96    47.42   27\n",
      "456  170     0.40   4.11      42.06  Present     56    33.10     2.06   57\n",
      "457  214     0.40   5.98      31.72   Absent     64    28.45     0.00   58\n",
      "458  182     4.20   4.41      32.10   Absent     52    28.61    18.72   52\n",
      "459  108     3.00   1.59      15.23   Absent     40    20.09    26.64   55\n",
      "460  118     5.40  11.61      30.79   Absent     64    27.35    23.97   40\n",
      "461  132     0.00   4.82      33.41  Present     62    14.70     0.00   46\n",
      "\n",
      "[462 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('heart.csv')\n",
    "print(raw_data.columns[:9])\n",
    "print(raw_data.loc[:,raw_data.columns[:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          sbp   tobacco       ldl  adiposity   famhist     typea   obesity  \\\n",
      "0    0.185241  0.268088  0.068967  -0.064244 -0.584416 -0.063137 -0.023341   \n",
      "1    0.048489 -0.116207 -0.023019   0.089602  0.415584  0.029171  0.088641   \n",
      "2   -0.173734 -0.113963 -0.087828   0.192259 -0.584416 -0.016983  0.097111   \n",
      "3    0.270711  0.123857  0.116354   0.353098 -0.584416 -0.032368  0.186508   \n",
      "4   -0.036982  0.319370 -0.086434   0.066385 -0.584416  0.106094 -0.001697   \n",
      "5   -0.054076  0.082191  0.120535   0.302189 -0.584416  0.136863  0.148240   \n",
      "6    0.031395  0.013280 -0.094796  -0.257531  0.415584  0.090709 -0.164182   \n",
      "7   -0.207922  0.014242 -0.010476  -0.302286 -0.584416  0.136863 -0.092036   \n",
      "8   -0.207922 -0.116527 -0.063437  -0.168020 -0.584416 -0.063137 -0.037143   \n",
      "9   -0.054076 -0.116527  0.073845   0.155336 -0.584416  0.244555  0.127537   \n",
      "10   0.578403  0.075780 -0.124761   0.191980  0.415584  0.290709  0.024024   \n",
      "11  -0.036982  0.335396 -0.020929  -0.084384 -0.584416  0.183017 -0.092664   \n",
      "12  -0.173734 -0.116527 -0.199326  -0.429559  0.415584  0.090709 -0.140342   \n",
      "13  -0.054076 -0.116527 -0.200023  -0.229279  0.415584 -0.063137 -0.075725   \n",
      "14  -0.225016  0.192768 -0.170754  -0.229559 -0.584416  0.013786 -0.078862   \n",
      "15  -0.182281 -0.067489 -0.160301   0.099112 -0.584416 -0.278521 -0.004834   \n",
      "16  -0.156640  0.123857  0.737956  -0.095293  0.415584  0.106094 -0.023027   \n",
      "17   0.065583  0.220011  0.247364   0.278413 -0.584416  0.383017  0.209720   \n",
      "18   0.168147 -0.033194  0.189524   0.242329 -0.584416  0.121479  0.102129   \n",
      "19  -0.122452  0.332191  0.103810   0.295196 -0.584416 -0.124675  0.126910   \n",
      "20  -0.276298 -0.064925 -0.209082  -0.366062  0.415584  0.321479 -0.160731   \n",
      "21  -0.054076  0.136678 -0.131730   0.030581 -0.584416 -0.032368  0.003635   \n",
      "22   0.099771 -0.106912  0.114263   0.240091 -0.584416  0.136863 -0.044044   \n",
      "23  -0.002794 -0.097296 -0.064831   0.091001  0.415584  0.013786  0.083309   \n",
      "24   0.031395  0.466806 -0.027897  -0.028720  0.415584  0.121479  0.004576   \n",
      "25  -0.122452  0.011678  0.535169   0.164567 -0.584416  0.013786 -0.088272   \n",
      "26  -0.173734  0.075780  0.342138   0.237854  0.415584  0.106094  0.400122   \n",
      "27   0.057036  0.175139  0.034821   0.059952  0.415584  0.090709 -0.159477   \n",
      "28   0.048489  0.014563  0.056423   0.167644 -0.584416  0.106094  0.106207   \n",
      "29   0.065583 -0.116527  0.130988   0.007924  0.415584  0.106094  0.063547   \n",
      "..        ...       ...       ...        ...       ...       ...       ...   \n",
      "432 -0.019888 -0.116527 -0.051591  -0.177531  0.415584 -0.201598 -0.128736   \n",
      "433 -0.156640 -0.116527 -0.158908  -0.336132  0.415584 -0.093906 -0.126541   \n",
      "434 -0.054076 -0.116527 -0.082949  -0.468440 -0.584416  0.121479 -0.236641   \n",
      "435 -0.019888 -0.116527 -0.206991  -0.140888  0.415584 -0.124675 -0.142224   \n",
      "436 -0.002794 -0.116527 -0.200719  -0.197391 -0.584416  0.090709 -0.020832   \n",
      "437 -0.002794 -0.114604 -0.041138  -0.132776  0.415584 -0.063137 -0.108347   \n",
      "438 -0.071170 -0.077425 -0.100371  -0.328860  0.415584 -0.047752 -0.145675   \n",
      "439 -0.071170  0.011678 -0.163089  -0.223405  0.415584  0.106094 -0.125286   \n",
      "440 -0.242110 -0.116527  0.167225   0.080371  0.415584  0.059940  0.092719   \n",
      "441 -0.156640 -0.116527 -0.052984  -0.341727 -0.584416 -0.093906 -0.130305   \n",
      "442  0.236523  0.075780  0.282904   0.349182  0.415584 -0.216983  0.083309   \n",
      "443 -0.036982 -0.098258  0.000674  -0.065363  0.415584  0.213786  0.008968   \n",
      "444  0.031395 -0.020373 -0.073193  -0.008580  0.415584  0.106094  0.126596   \n",
      "445 -0.019888 -0.026784 -0.154030  -0.451097 -0.584416  0.121479 -0.167632   \n",
      "446  0.031395 -0.116527 -0.029291  -0.005223  0.415584 -0.093906  0.090210   \n",
      "447 -0.071170 -0.116527 -0.199326  -0.360748 -0.584416 -0.016983 -0.180807   \n",
      "448 -0.122452 -0.058835 -0.069709  -0.245223 -0.584416 -0.170829 -0.118699   \n",
      "449  0.048489  0.011678  0.020186   0.010441 -0.584416  0.059940  0.047236   \n",
      "450 -0.019888 -0.058514 -0.099674  -0.522146  0.415584  0.152248 -0.203078   \n",
      "451 -0.156640 -0.116527 -0.137305  -0.337251  0.415584  0.213786 -0.083881   \n",
      "452  0.133959  0.060716 -0.107340   0.095196 -0.584416  0.121479  0.003321   \n",
      "453 -0.122452 -0.065245  0.172800   0.399252 -0.584416 -0.263137  0.171138   \n",
      "454  0.065583 -0.096014  0.005552   0.073098  0.415584  0.106094  0.064802   \n",
      "455 -0.088264 -0.044732 -0.133124   0.030021  0.415584 -0.078521 -0.065374   \n",
      "456  0.270711 -0.103707 -0.043925   0.465826 -0.584416  0.044555  0.221326   \n",
      "457  0.646779 -0.103707  0.086389   0.176595  0.415584  0.167632  0.075467   \n",
      "458  0.373275  0.018088 -0.023019   0.187224  0.415584 -0.016983  0.080486   \n",
      "459 -0.259204 -0.020373 -0.219535  -0.284664  0.415584 -0.201598 -0.186766   \n",
      "460 -0.173734  0.056550  0.478723   0.150581  0.415584  0.167632  0.040963   \n",
      "461 -0.054076 -0.116527  0.005552   0.223868 -0.584416  0.136863 -0.355838   \n",
      "\n",
      "      alcohol       age  chd  \n",
      "0    0.544572  0.187428    1  \n",
      "1   -0.101803  0.411918    1  \n",
      "2   -0.089914  0.064979    0  \n",
      "3    0.049022  0.309877    1  \n",
      "4    0.273766  0.126204    1  \n",
      "5   -0.019732  0.044571    0  \n",
      "6   -0.097998 -0.098286    0  \n",
      "7   -0.070143  0.309877    1  \n",
      "8   -0.098882 -0.281960    0  \n",
      "9   -0.115799  0.207836    1  \n",
      "10   0.265070  0.350694    1  \n",
      "11  -0.115799 -0.057470    1  \n",
      "12  -0.115799 -0.526857    0  \n",
      "13  -0.109208 -0.567674    0  \n",
      "14  -0.111179  0.207836    0  \n",
      "15   0.088223  0.064979    0  \n",
      "16   0.118524  0.126204    0  \n",
      "17  -0.021431  0.207836    1  \n",
      "18   0.246183  0.391510    1  \n",
      "19  -0.115799  0.330285    1  \n",
      "20  -0.024964 -0.465633    1  \n",
      "21   0.058874  0.024163    0  \n",
      "22  -0.115799  0.146612    0  \n",
      "23  -0.105879  0.309877    0  \n",
      "24  -0.115799  0.146612    0  \n",
      "25  -0.101803 -0.016653    1  \n",
      "26  -0.115799  0.105796    0  \n",
      "27   0.030951  0.371102    1  \n",
      "28  -0.078092  0.269061    0  \n",
      "29  -0.059884  0.411918    1  \n",
      "..        ...       ...  ...  \n",
      "432 -0.101803 -0.547266    0  \n",
      "433 -0.112334 -0.506449    0  \n",
      "434 -0.089506 -0.547266    0  \n",
      "435 -0.101803 -0.547266    0  \n",
      "436 -0.071570 -0.526857    0  \n",
      "437 -0.098882 -0.547266    0  \n",
      "438 -0.089914 -0.241143    0  \n",
      "439 -0.115799 -0.057470    0  \n",
      "440 -0.115799 -0.220735    0  \n",
      "441 -0.115799 -0.547266    0  \n",
      "442  0.177700  0.187428    0  \n",
      "443 -0.115799 -0.118694    0  \n",
      "444  0.148350 -0.322776    0  \n",
      "445 -0.084886 -0.363592    0  \n",
      "446 -0.071434 -0.179919    1  \n",
      "447 -0.115799 -0.526857    0  \n",
      "448 -0.044530 -0.465633    0  \n",
      "449  0.495656  0.105796    1  \n",
      "450  0.053642 -0.384000    0  \n",
      "451 -0.108801 -0.506449    0  \n",
      "452  0.174914 -0.016653    0  \n",
      "453 -0.115799  0.167020    1  \n",
      "454 -0.059884 -0.077878    1  \n",
      "455  0.206370 -0.322776    1  \n",
      "456 -0.101803  0.289469    0  \n",
      "457 -0.115799  0.309877    0  \n",
      "458  0.011384  0.187428    1  \n",
      "459  0.065192  0.248653    0  \n",
      "460  0.047052 -0.057470    0  \n",
      "461 -0.115799  0.064979    1  \n",
      "\n",
      "[462 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Pre-treatment\n",
    "raw_data = pd.read_csv('heart.csv')\n",
    "\n",
    "label = raw_data['chd']\n",
    "\n",
    "data = raw_data.loc[:,raw_data.columns[:9]].replace(['Present', 'Absent'],[0, 1])\n",
    "data = data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "all_data = data\n",
    "all_data['chd'] = label\n",
    "all_data.describe()\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sbp</th>\n",
       "      <th>tobacco</th>\n",
       "      <th>ldl</th>\n",
       "      <th>adiposity</th>\n",
       "      <th>famhist</th>\n",
       "      <th>typea</th>\n",
       "      <th>obesity</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>age</th>\n",
       "      <th>chd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006581</td>\n",
       "      <td>0.011118</td>\n",
       "      <td>0.018841</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>-0.056291</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>-0.000335</td>\n",
       "      <td>0.040298</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.183460</td>\n",
       "      <td>0.155476</td>\n",
       "      <td>0.151145</td>\n",
       "      <td>0.210646</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>0.156828</td>\n",
       "      <td>0.138021</td>\n",
       "      <td>0.163842</td>\n",
       "      <td>0.282190</td>\n",
       "      <td>0.500783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.310486</td>\n",
       "      <td>-0.116527</td>\n",
       "      <td>-0.255772</td>\n",
       "      <td>-0.522146</td>\n",
       "      <td>-0.584416</td>\n",
       "      <td>-0.616983</td>\n",
       "      <td>-0.355838</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>-0.567674</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.122452</td>\n",
       "      <td>-0.107553</td>\n",
       "      <td>-0.083646</td>\n",
       "      <td>-0.124034</td>\n",
       "      <td>-0.584416</td>\n",
       "      <td>-0.082368</td>\n",
       "      <td>-0.087409</td>\n",
       "      <td>-0.115799</td>\n",
       "      <td>-0.179919</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.036982</td>\n",
       "      <td>-0.045373</td>\n",
       "      <td>-0.009082</td>\n",
       "      <td>0.029882</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.013786</td>\n",
       "      <td>-0.003893</td>\n",
       "      <td>-0.062704</td>\n",
       "      <td>0.075183</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.099771</td>\n",
       "      <td>0.075780</td>\n",
       "      <td>0.091963</td>\n",
       "      <td>0.167644</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.106094</td>\n",
       "      <td>0.075781</td>\n",
       "      <td>0.050008</td>\n",
       "      <td>0.274163</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680967</td>\n",
       "      <td>0.883473</td>\n",
       "      <td>0.737956</td>\n",
       "      <td>0.477854</td>\n",
       "      <td>0.415584</td>\n",
       "      <td>0.383017</td>\n",
       "      <td>0.644162</td>\n",
       "      <td>0.884201</td>\n",
       "      <td>0.432326</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sbp     tobacco         ldl   adiposity     famhist       typea  \\\n",
       "count  320.000000  320.000000  320.000000  320.000000  320.000000  320.000000   \n",
       "mean     0.006581    0.011118    0.018841    0.015173   -0.056291    0.007055   \n",
       "std      0.183460    0.155476    0.151145    0.210646    0.499990    0.156828   \n",
       "min     -0.310486   -0.116527   -0.255772   -0.522146   -0.584416   -0.616983   \n",
       "25%     -0.122452   -0.107553   -0.083646   -0.124034   -0.584416   -0.082368   \n",
       "50%     -0.036982   -0.045373   -0.009082    0.029882    0.415584    0.013786   \n",
       "75%      0.099771    0.075780    0.091963    0.167644    0.415584    0.106094   \n",
       "max      0.680967    0.883473    0.737956    0.477854    0.415584    0.383017   \n",
       "\n",
       "          obesity     alcohol         age         chd  \n",
       "count  320.000000  320.000000  320.000000  320.000000  \n",
       "mean     0.005834   -0.000335    0.040298    0.500000  \n",
       "std      0.138021    0.163842    0.282190    0.500783  \n",
       "min     -0.355838   -0.115799   -0.567674    0.000000  \n",
       "25%     -0.087409   -0.115799   -0.179919    0.000000  \n",
       "50%     -0.003893   -0.062704    0.075183    0.500000  \n",
       "75%      0.075781    0.050008    0.274163    1.000000  \n",
       "max      0.644162    0.884201    0.432326    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重组数据集，保证label数量相等\n",
    "one_label_result = all_data[(all_data.chd == 1)]\n",
    "zero_label_result = all_data[(all_data.chd == 0)]\n",
    "\n",
    "one_label_length = len(one_label_result)\n",
    "zero_label_length = len(zero_label_result)\n",
    "\n",
    "small_len = one_label_length if one_label_length < zero_label_length else zero_label_length;\n",
    "\n",
    "\n",
    "one_index = random.sample(list(one_label_result.index.values), small_len)\n",
    "zero_index = random.sample(list(zero_label_result.index.values), small_len)\n",
    "\n",
    "new_data = pd.concat([one_label_result.ix[one_index], zero_label_result.ix[zero_index]])\n",
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 9) (288,)\n",
      "(32, 9) (32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 数据分10份，拿一份做测试集，九份做训练集\n",
    "train_data_size = int(small_len * 2 * 0.9)\n",
    "test_data_size = int(small_len * 2 * 0.1)\n",
    "\n",
    "train_data_index = random.sample(list(new_data.index.values), train_data_size)\n",
    "train_data = new_data.ix[train_data_index]\n",
    "\n",
    "test_data_index = list(set(new_data.index.values).difference(set(train_data_index)))\n",
    "test_data = new_data.ix[test_data_index]\n",
    "\n",
    "train_label = train_data['chd']\n",
    "train_data = train_data.loc[:,raw_data.columns[:9]]\n",
    "\n",
    "test_label = test_data['chd']\n",
    "test_data = test_data.loc[:,raw_data.columns[:9]]\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch :1.1135838892724779\n",
      "Average loss epoch :0.19870074755615658\n",
      "Average loss epoch :0.11422936318235265\n",
      "Average loss epoch :0.022384324421485264\n",
      "Average loss epoch :0.004669598190553693\n",
      "Average loss epoch :0.001290346626774408\n",
      "Average loss epoch :0.00040103785431711003\n",
      "Average loss epoch :0.00013122664789635665\n",
      "Average loss epoch :4.6420002427617467e-05\n",
      "Average loss epoch :1.7395373327468e-05\n",
      "Total time: 14.735323429107666 seconds\n",
      "Optimization Finished!\n",
      "Accuracy: 0.53125\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(dtype = np.float32, shape = [batch_size, 9], name='X')\n",
    "Y = tf.placeholder(dtype = np.float32, shape = [batch_size, 2], name='Y')\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([9, 20]), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([1, 20]), name='b1')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([20, 10]), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1, 10]), name='b2')\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 2]), name='W3')\n",
    "b3 = tf.Variable(tf.random_normal([1, 2]), name='b3')\n",
    "\n",
    "Z1 = tf.matmul(X, W1) + b1\n",
    "A1 = tf.nn.relu(Z1)\n",
    "Z2 = tf.matmul(A1, W2) + b2\n",
    "A2 = tf.nn.relu(Z2)\n",
    "logits = tf.matmul(A2, W3) + b3\n",
    "\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "loss = tf.reduce_mean(entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "def to_one_hotting(labels):\n",
    "    return (np.arange(2) == labels[:,None]).astype(np.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\t\n",
    "    n_batches = int(len(train_data)/batch_size)\n",
    "    for i in range(n_epochs): \n",
    "        total_loss = 0\n",
    "\n",
    "        for index in range(n_batches):\n",
    "            X_batch = train_data[index*batch_size:(index+1)*batch_size].values\n",
    "            Y_batch = train_label[index*batch_size:(index+1)*batch_size].values\n",
    "            Y_batch = to_one_hotting(Y_batch)\n",
    "            _, loss_batch, get_entropy, get_logits = sess.run([optimizer, loss, entropy, logits], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += loss_batch\n",
    "            \n",
    "        if i%100 == 0:\n",
    "            print('Average loss epoch :{0}'.format(total_loss/n_batches))\n",
    "\n",
    "    print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "\n",
    "    print('Optimization Finished!')\n",
    "\n",
    "    # test the model\n",
    "    n_batches = int(len(test_data)/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for index in range(n_batches):\n",
    "        X_batch = test_data[index*batch_size:(index+1)*batch_size].values\n",
    "        Y_batch = test_label[index*batch_size:(index+1)*batch_size].values\n",
    "        Y_batch = to_one_hotting(Y_batch)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "\n",
    "    print('Accuracy:',format(total_correct_preds/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
