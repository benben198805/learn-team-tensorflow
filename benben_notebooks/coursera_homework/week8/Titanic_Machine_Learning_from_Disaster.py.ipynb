{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from tf_utils import random_mini_batches\n",
    "\n",
    "seed = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preTreatmentCabin(x):\n",
    "    try:\n",
    "        isinstance(float(x),(float))\n",
    "        return '0'\n",
    "    except:\n",
    "        return x[0]\n",
    "    \n",
    "def preTreatmentData(treatment_data, with_label = True):\n",
    "    # sex\n",
    "    treatment_data['Sex'] = treatment_data['Sex'].replace(['male', 'female'],[0, 1])\n",
    "\n",
    "    # age\n",
    "    average_age = np.average(list(filter(lambda x: not np.isnan(x), treatment_data['Age'])))\n",
    "    treatment_data['Age'] = np.nan_to_num(treatment_data['Age'], average_age)\n",
    "\n",
    "    # Embarked\n",
    "    data_embarked_unique = {}.fromkeys(treatment_data['Embarked']).keys()\n",
    "    treatment_data['Embarked'] = treatment_data['Embarked'].replace(data_embarked_unique, np.arange(len(data_embarked_unique)))\n",
    "\n",
    "    # Cabin\n",
    "    data_cabin_string_list = list(map(preTreatmentCabin, list(treatment_data['Cabin'])))\n",
    "    data_cabin_keys = {}.fromkeys(data_cabin_string_list).keys()\n",
    "    treatment_data['Cabin'] = data_cabin_string_list\n",
    "    treatment_data['Cabin'] = treatment_data['Cabin'].replace(data_cabin_keys, np.arange(len(data_cabin_keys)))\n",
    "\n",
    "    # print(data)\n",
    "    model_data = treatment_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']]\n",
    "    pre_treatment_data = model_data.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)), axis=0)\n",
    "\n",
    "    if with_label:\n",
    "        pre_treatment_data['Survived'] = treatment_data['Survived']\n",
    "    \n",
    "    return pre_treatment_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(train_data, train_label, test_data, test_label, \n",
    "         learning_rate = 0.005, batch_size = 32, \n",
    "         n_epochs = 1000, seed = 3,\n",
    "         threshold = 0.9, show_epoch_cost = False,\n",
    "         is_train = True\n",
    "        ):\n",
    "    X = tf.placeholder(dtype = np.float32, shape = [8, None])\n",
    "    Y = tf.placeholder(dtype = np.float32, shape = [1, None])\n",
    "\n",
    "    W1 = tf.Variable(tf.random_normal([20, 8]), name='W1')\n",
    "    b1 = tf.Variable(tf.random_normal([20, 1]), name='b1')\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([10, 20]), name='W2')\n",
    "    b2 = tf.Variable(tf.random_normal([10, 1]), name='b2')\n",
    "\n",
    "    W3 = tf.Variable(tf.random_normal([1, 10]), name='W3')\n",
    "    b3 = tf.Variable(tf.random_normal([1, 1]), name='b3')\n",
    "\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1) \n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2) \n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    logits = tf.add(tf.matmul(W3, A2), b3) \n",
    "\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "\n",
    "    cost = tf.reduce_mean(entropy)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        summary_writer = tf.summary.FileWriter('../tmp', sess.graph)\n",
    "        start_time = time.time()\n",
    "        sess.run(tf.global_variables_initializer())\t\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "            epoch_cost = 0        \n",
    "            n_batches = int(train_set.shape[1]/batch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(train_set, train_label, batch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / n_batches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if epoch % 100 == 0 and show_epoch_cost:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if epoch % 5 == 0 and show_epoch_cost:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        print('Total time: {0} seconds'.format(time.time() - start_time))\n",
    "        print('Optimization Finished!')\n",
    "#         print('----------------------')\n",
    "\n",
    "#         print('params: ')\n",
    "\n",
    "        W1 = sess.run(W1)\n",
    "        b1 = sess.run(b1)\n",
    "        W2 = sess.run(W2)\n",
    "        b2 = sess.run(b2)\n",
    "        W3 = sess.run(W3)\n",
    "        b3 = sess.run(b3)\n",
    "\n",
    "    #     print('W1', sess.run(W1))\n",
    "    #     print('b1', sess.run(b1))\n",
    "    #     print('W2', sess.run(W2))\n",
    "    #     print('b2', sess.run(b2))\n",
    "    #     print('W3', sess.run(W3))\n",
    "    #     print('b3', sess.run(b3))\n",
    "#         print('----------------------')\n",
    "\n",
    "        print('test the model:')\n",
    "\n",
    "        result = tf.cast(tf.greater(logits, threshold), \"float\")\n",
    "        correct_prediction = tf.equal(result , Y)\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_set, Y: train_label}))\n",
    "        if is_train:\n",
    "            print (\"Test Accuracy:\", accuracy.eval({X: test_set, Y: test_label}))\n",
    "        else: \n",
    "            predict_label = result.eval({X: test_set})\n",
    "            print (\"result Y:\", predict_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (891, 9)\n",
      "train_data shape: (712, 9)\n",
      "test_data shape: (179, 9)\n",
      "train_set shape: (8, 712)\n",
      "train_label shape: (1, 712)\n",
      "test_set shape: (8, 179)\n",
      "test_label shape: (1, 179)\n"
     ]
    }
   ],
   "source": [
    "# train_data\n",
    "raw_train_data = pd.read_csv('train.csv')\n",
    "train_data = preTreatmentData(raw_train_data)\n",
    "\n",
    "# test_data\n",
    "# raw_test_data = pd.read_csv('test.csv')\n",
    "\n",
    "data = train_data.values\n",
    "print(\"data shape:\", data.shape)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "\n",
    "train_set = train_data[:, 0:8].T\n",
    "train_label = train_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"train_set shape:\", train_set.shape)\n",
    "print(\"train_label shape:\", train_label.shape)\n",
    "\n",
    "# batches = random_mini_batches(train_set, train_label, batch_size, seed)\n",
    "# print(batches[0][0].shape)\n",
    "# print(batches[0][1].shape)\n",
    "\n",
    "\n",
    "test_set = test_data[:, 0:8].T\n",
    "test_label = test_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"test_set shape:\", test_set.shape)\n",
    "print(\"test_label shape:\", test_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 3.189983\n",
      "Cost after epoch 100: 0.350952\n",
      "Cost after epoch 200: 0.305264\n",
      "Cost after epoch 300: 0.288847\n",
      "Cost after epoch 400: 0.307505\n",
      "Cost after epoch 500: 0.277612\n",
      "Cost after epoch 600: 0.274265\n",
      "Cost after epoch 700: 0.288779\n",
      "Cost after epoch 800: 0.284886\n",
      "Cost after epoch 900: 0.268920\n",
      "Total time: 14.816195011138916 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.890449\n",
      "Test Accuracy: 0.782123\n"
     ]
    }
   ],
   "source": [
    "mode(train_data, train_label, test_data, test_label, show_epoch_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> model with learning rate:  0.5\n",
      "Total time: 28.966325521469116 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.794944\n",
      "Test Accuracy: 0.75419\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.1\n",
      "Total time: 33.98376822471619 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.766854\n",
      "Test Accuracy: 0.72067\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.05\n",
      "Total time: 27.685404062271118 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.860955\n",
      "Test Accuracy: 0.743017\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.01\n",
      "Total time: 27.273911237716675 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.924157\n",
      "Test Accuracy: 0.77095\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.005\n",
      "Total time: 27.4489688873291 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.929775\n",
      "Test Accuracy: 0.75419\n",
      "-----------------------------------\n",
      ">>> model with learning rate:  0.001\n",
      "Total time: 27.493070125579834 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.882023\n",
      "Test Accuracy: 0.765363\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.5, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(\">>> model with learning rate: \", lr)\n",
    "    mode(train_data, train_label, test_data, test_label, learning_rate = lr, n_epochs=2000)\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (891, 9)\n",
      "train_data shape: (712, 9)\n",
      "test_data shape: (179, 9)\n",
      "train_set shape: (8, 712)\n",
      "train_label shape: (1, 712)\n",
      "test_set shape: (8, 418)\n"
     ]
    }
   ],
   "source": [
    "# train_data\n",
    "raw_train_data = pd.read_csv('train.csv')\n",
    "train_data = preTreatmentData(raw_train_data)\n",
    "\n",
    "data = train_data.values\n",
    "print(\"data shape:\", data.shape)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "print(\"train_data shape:\", train_data.shape)\n",
    "print(\"test_data shape:\", test_data.shape)\n",
    "\n",
    "train_set = train_data[:, 0:8].T\n",
    "train_label = train_data[:, 8].reshape((-1, 1)).T\n",
    "print(\"train_set shape:\", train_set.shape)\n",
    "print(\"train_label shape:\", train_label.shape)\n",
    "\n",
    "# test_data\n",
    "# raw_test_data = pd.read_csv('test.csv')\n",
    "test_data = preTreatmentData(raw_test_data, with_label = False)\n",
    "\n",
    "test_set = test_data.T\n",
    "print(\"test_set shape:\", test_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 29.322301149368286 seconds\n",
      "Optimization Finished!\n",
      "test the model:\n",
      "Train Accuracy: 0.896067\n",
      "result Y: [[ 0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  1.\n",
      "   0.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.\n",
      "   1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.\n",
      "   1.  0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.\n",
      "   0.  0.  1.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  0.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.  1.\n",
      "   0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1.\n",
      "   1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  1.  1.  1.  1.  0.\n",
      "   0.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  1.\n",
      "   0.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  0.  0.  1.  0.\n",
      "   1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.\n",
      "   1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  0.\n",
      "   1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0.  0.\n",
      "   1.  0.  1.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  1.\n",
      "   1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      "   1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.\n",
      "   0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0.\n",
      "   1.  1.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "mode(train_data, train_label, test_data, test_label, learning_rate = 0.001, n_epochs=2000, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_survived = np.array([ 0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  1,  1,  0,  1,\n",
    "   0,  1,  0,  1,  1,  0,  1,  0,  1,  1,  0,  1,  0,  0,  0,  0,  1,  1,\n",
    "   1,  1,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  0,  1,  1,\n",
    "   1,  0,  1,  0,  0,  1,  0,  0,  0,  1,  0,  1,  1,  0,  0,  1,  1,  0,\n",
    "   0,  0,  1,  0,  0,  1,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1,  1,  1,\n",
    "   0,  0,  1,  0,  0,  0,  1,  0,  1,  1,  1,  0,  0,  0,  1,  0,  0,  0,\n",
    "   0,  1,  0,  1,  1,  1,  1,  1,  0,  0,  0,  1,  1,  0,  1,  0,  0,  1,\n",
    "   0,  1,  0,  0,  0,  0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  1,  0,  0,\n",
    "   0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,  1,  1,\n",
    "   1,  0,  0,  0,  1,  0,  1,  0,  0,  1,  0,  0,  0,  1,  1,  1,  1,  0,\n",
    "   0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,  1,  0,  0,  1,  1,  0,  1,\n",
    "   0,  1,  1,  0,  0,  1,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  1,  0,\n",
    "   1,  0,  1,  0,  1,  0,  1,  0,  1,  1,  0,  1,  1,  0,  1,  1,  0,  0,\n",
    "   1,  0,  0,  1,  1,  1,  1,  1,  0,  0,  0,  0,  1,  0,  1,  1,  1,  0,\n",
    "   1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,  0,  1,  0,  1,  0,\n",
    "   0,  0,  1,  1,  0,  1,  0,  0,  0,  0,  0,  1,  1,  1,  0,  1,  0,  0,\n",
    "   1,  0,  1,  1,  0,  0,  1,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  1,\n",
    "   1,  0,  0,  0,  0,  1,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  0,  1,\n",
    "   1,  0,  1,  0,  0,  0,  1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,\n",
    "   0,  1,  1,  1,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
    "   0,  1,  1,  0,  1,  0,  1,  1,  1,  0,  0,  1,  0,  0,  1,  1,  0,  0,\n",
    "   0,  0,  0,  0,  1,  0,  1,  1,  0,  0,  0,  0,  0,  1,  1,  0,  0,  1,\n",
    "   0,  1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  1,  0,  1,  1,  0,  0,\n",
    "   1,  1,  0,  1])\n",
    "\n",
    "predict_label = pd.DataFrame()\n",
    "predict_label['Survived'] = is_survived\n",
    "predict_label['PassengerId'] = raw_test_data['PassengerId']\n",
    "predict_label.to_csv('predict_label.csv',index=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
